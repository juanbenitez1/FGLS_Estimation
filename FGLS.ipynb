{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Finite sample properties of FGLS**"
      ],
      "metadata": {
        "id": "Yr73mEPhOpZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us estimate using the Feasible Generalised Least Squares method the following model:\n",
        "\n",
        "y_i=β_0+β_1 x_1+u_i with i=1,2,...,5N\n",
        "\n",
        "Where:\n",
        "\n",
        "  β_0=-3\n",
        "\n",
        "  β_1=0.8\n",
        "\n",
        "  u_j ~ N(0,Ω⨂I_NxN)\n",
        "\n",
        "  Ω is a 5x5 diagonal matrix whose main diagonal elements are 4, 9, 16, 25 and 36.\n",
        "\n",
        "  x_j ~ U(1,50)\n",
        "\n",
        "  To estimate the model, 5000 samples have been generated for the number of observations 5N equal to 5, 10, 30, 200 and 500.\n",
        "\n",
        "The application of the FGLS method involves a number of steps:\n",
        "1.   Estimate the model for each sample by Classical Least Squares.\n",
        "2.   Extract its residuals and transform them\n",
        "3.   Estimate a CQLS model using the residuals extracted in step 2 as the dependent variable.\n",
        "4.   Extract the predicted values and transform them to use them as weights.\n",
        "5.   Estimate by Weighted Least Squares the model estimated in step 1, using the weights transformed in step 4 as weights."
      ],
      "metadata": {
        "id": "6X933jXMPIDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "aEmKk2qCP9pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import t, norm\n",
        "from numpy.linalg import inv, cholesky"
      ],
      "metadata": {
        "id": "fky-LvImP-dg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "USUgV62cX5it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed\n",
        "np.random.seed(4678)\n",
        "\n",
        "# Parameters\n",
        "N = [1, 2, 6, 20, 40, 100]\n",
        "num_simulations = 5000\n",
        "results_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "Lb7Bl2PDOqvx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lists for storing the results"
      ],
      "metadata": {
        "id": "LXInzj0YYBdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimated coefficients FGLS\n",
        "beta_0_estimates = {j: [] for j in N}\n",
        "beta_1_estimates = {j: [] for j in N}\n",
        "\n",
        "# Estimated coefficients MCG Cholesky\n",
        "beta_0_estimates_chol = {j: [] for j in N}\n",
        "beta_1_estimates_chol = {j: [] for j in N}\n",
        "\n",
        "# FGLS standard deviations\n",
        "beta_0_std = {j: [] for j in N}\n",
        "beta_1_std = {j: [] for j in N}\n",
        "\n",
        "# MCG Cholesky standard deviations\n",
        "beta_0_std_chol = {j: [] for j in N}\n",
        "beta_1_std_chol = {j: [] for j in N}\n",
        "\n",
        "# Test Size and Power Size\n",
        "test_size_1 = {j: [] for j in N}\n",
        "test_size_5 = {j: [] for j in N}\n",
        "test_power_0_1 = {j: [] for j in N}\n",
        "test_power_0_5 = {j: [] for j in N}\n",
        "test_power_04_1 = {j: [] for j in N}\n",
        "test_power_04_5 = {j: [] for j in N}"
      ],
      "metadata": {
        "id": "dSFVn9UfYE5n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation"
      ],
      "metadata": {
        "id": "5YsTE3SoYjef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in N:\n",
        "    Omega = np.diag(np.tile([4, 9, 16, 25, 36], n))\n",
        "\n",
        "    # Cholesky Decomposition\n",
        "    P = cholesky(inv(Omega))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    rechazos_tf1_95 = 0\n",
        "    rechazos_tf1_99 = 0\n",
        "\n",
        "    rechazos_tf2_95 = 0\n",
        "    rechazos_tf2_99 = 0\n",
        "\n",
        "    rechazos_tf3_95 = 0\n",
        "    rechazos_tf3_99 = 0\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        # Generate values for X and U\n",
        "        X = np.column_stack((np.ones(n*5), np.random.uniform(1, 50, n*5)))\n",
        "        U = np.random.normal(0, np.tile([2, 3, 4, 5, 6], n), n*5)\n",
        "\n",
        "        # Generate our Y model\n",
        "        Y = X @ np.array([-3, .8]) + U\n",
        "\n",
        "        # OLS Estimation\n",
        "        model = sm.OLS(Y, X).fit()\n",
        "\n",
        "        # Extract the residuals and square their logarithm\n",
        "        model_residuals = model.resid\n",
        "        logRes2 = np.log(model_residuals**2)\n",
        "\n",
        "        \"\"\"\n",
        "        Estimate by OLS a model that uses as dependent variable the residuals\n",
        "        that we store in logRes2. Then extract the predicted values that will\n",
        "        be used to calculate the weights.\n",
        "        \"\"\"\n",
        "        logRes2_Hat = sm.OLS(logRes2, X).fit().fittedvalues\n",
        "\n",
        "        # Transforming weights\n",
        "        weight = 1 / np.sqrt(np.exp(logRes2_Hat))\n",
        "\n",
        "        # Estimate by Weighted Least Squares the model, using \"weight\" as the weight.\n",
        "        wls_model = sm.WLS(Y, X, weights=weight).fit()\n",
        "\n",
        "        # Extract the standard deviation that will allow us to calculate the t-statistics.\n",
        "        std_FGLS = wls_model.bse[1]\n",
        "\n",
        "        # Calculate the t-statistics to test the hypothesis of B_0.\n",
        "        Tstat_B1_wls = abs(wls_model.params[1] - 0.8) / std_FGLS\n",
        "        Tstat_B1_wls_1 = abs(wls_model.params[1]) / std_FGLS\n",
        "        Tstat_B1_wls_2 = abs(wls_model.params[1] - 0.4) / std_FGLS\n",
        "\n",
        "        \"\"\"\n",
        "        We found the matrix P at the beginning by using the de Cholesky decomposition.\n",
        "        We now use P to calculate the variables X_chol and Y_chol.\n",
        "        \"\"\"\n",
        "        # Pre-multiply X and Y by P\n",
        "        X_chol = P @ X\n",
        "        Y_chol = P @ Y\n",
        "\n",
        "        # Estimate by OLS a model whose dependent variable is Y_chol and explanatory variable X_chol.\n",
        "        ols_model_chol = sm.OLS(Y_chol, X_chol).fit()\n",
        "\n",
        "        # Extract the deviation from the model\n",
        "        std_chol = ols_model_chol.bse[1]\n",
        "\n",
        "        # Store estimated coefficients FGLS\n",
        "        beta_0_estimates[n].append(wls_model.params[0])\n",
        "        beta_1_estimates[n].append(wls_model.params[1])\n",
        "\n",
        "        # Store FGLS deviations\n",
        "        beta_0_std[n].append(wls_model.bse[0])\n",
        "        beta_1_std[n].append(wls_model.bse[1])\n",
        "\n",
        "        # Store estimated coefficients MCG Cholesky\n",
        "        beta_0_estimates_chol[n].append(ols_model_chol.params[0])\n",
        "        beta_1_estimates_chol[n].append(ols_model_chol.params[1])\n",
        "\n",
        "        # Store MCG Cholesky deviations\n",
        "        beta_0_std_chol[n].append(ols_model_chol.bse[0])\n",
        "        beta_1_std_chol[n].append(ols_model_chol.bse[1])\n",
        "\n",
        "        if Tstat_B1_wls > t.ppf(0.995, n*5-2):\n",
        "          rechazos_tf1_99 += 1\n",
        "        if Tstat_B1_wls > t.ppf(0.975, n*5-2):\n",
        "          rechazos_tf1_95 +=1\n",
        "\n",
        "\n",
        "        if Tstat_B1_wls_1 > t.ppf(0.995, n*5-2):\n",
        "          rechazos_tf2_99 += 1\n",
        "        if Tstat_B1_wls_1 > t.ppf(0.975, n*5-2):\n",
        "          rechazos_tf2_95 +=1\n",
        "\n",
        "\n",
        "        if Tstat_B1_wls_2 > t.ppf(0.995, n*5-2):\n",
        "          rechazos_tf3_99 += 1\n",
        "        if Tstat_B1_wls_2 > t.ppf(0.975, n*5-2):\n",
        "          rechazos_tf3_95 +=1\n",
        "\n",
        "\n",
        "    # Store Test Size and Test Power\n",
        "    test_size_1[n].append(rechazos_tf1_99/5000)\n",
        "    test_size_5[n].append(rechazos_tf1_95/5000)\n",
        "    test_power_0_1[n].append(rechazos_tf2_99/5000)\n",
        "    test_power_0_5[n].append(rechazos_tf2_95/5000)\n",
        "    test_power_04_1[n].append(rechazos_tf3_99/5000)\n",
        "    test_power_04_5[n].append(rechazos_tf3_95/5000)"
      ],
      "metadata": {
        "id": "eeoxeud1Yrvw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Size and Test Power"
      ],
      "metadata": {
        "id": "lFHxMFV3Ysag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe with the results of the test sizes and power\n",
        "test_results_df = pd.DataFrame(index=N, columns=['Test_Size_1', 'Test_Size_5', 'Power_Beta_0_1', 'Power_Beta_0_5', 'Power_Beta_0.4_1', 'Power_Beta_0.4_5'])\n",
        "\n",
        "for j in N:\n",
        "    test_results_df.loc[j, 'Test_Size_1'] = np.mean(test_size_1[j])\n",
        "    test_results_df.loc[j, 'Test_Size_5'] = np.mean(test_size_5[j])\n",
        "    test_results_df.loc[j, 'Power_Beta_0_1'] = np.mean(test_power_0_1[j])\n",
        "    test_results_df.loc[j, 'Power_Beta_0_5'] = np.mean(test_power_0_5[j])\n",
        "    test_results_df.loc[j, 'Power_Beta_0.4_1'] = np.mean(test_power_04_1[j])\n",
        "    test_results_df.loc[j, 'Power_Beta_0.4_5'] = np.mean(test_power_04_5[j])\n",
        "\n",
        "print(test_results_df)\n",
        "excel_filename = 'sizes_power.xlsx'\n",
        "test_results_df.to_excel(excel_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsh7W4WeY2kQ",
        "outputId": "e14fc330-eb1a-4d79-84df-eb81736f52e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Test_Size_1 Test_Size_5 Power_Beta_0_1 Power_Beta_0_5 Power_Beta_0.4_1  \\\n",
            "1        0.0184      0.0788          0.597         0.8876           0.2086   \n",
            "2         0.014      0.0634         0.9916         0.9988           0.7254   \n",
            "6        0.0126      0.0532            1.0            1.0           0.9992   \n",
            "20       0.0114      0.0496            1.0            1.0              1.0   \n",
            "40       0.0112      0.0554            1.0            1.0              1.0   \n",
            "100      0.0078      0.0454            1.0            1.0              1.0   \n",
            "\n",
            "    Power_Beta_0.4_5  \n",
            "1             0.5406  \n",
            "2             0.9074  \n",
            "6                1.0  \n",
            "20               1.0  \n",
            "40               1.0  \n",
            "100              1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical measures of estimated Beta FGLS"
      ],
      "metadata": {
        "id": "bLHInQ4VY5GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MultiIndex for the Beta FGLS coefficient columns.\n",
        "# This will allow grouping \"Mean\", \"Median\" and \"Deviation\" under Beta_0 and Beta_1.\n",
        "beta_columns = pd.MultiIndex.from_product([['Beta_0', 'Beta_1'], ['Media', 'Mediana', 'Desvio']],\n",
        "                                           names=['', 'N'])\n",
        "\n",
        "# Create a DataFrame to store the results of the Beta FGLS coefficients.\n",
        "beta_results_df = pd.DataFrame(index=N, columns=beta_columns)\n",
        "\n",
        "for j in N:\n",
        "    beta_results_df.loc[j, ('Beta_0', 'Media')] = np.mean(beta_0_estimates[j])\n",
        "    beta_results_df.loc[j, ('Beta_0', 'Mediana')] = np.median(beta_0_estimates[j])\n",
        "    beta_results_df.loc[j, ('Beta_0', 'Desvio')] = np.mean(beta_0_std[j])\n",
        "\n",
        "    beta_results_df.loc[j, ('Beta_1', 'Media')] = np.mean(beta_1_estimates[j])\n",
        "    beta_results_df.loc[j, ('Beta_1', 'Mediana')] = np.median(beta_1_estimates[j])\n",
        "    beta_results_df.loc[j, ('Beta_1', 'Desvio')] = np.mean(beta_1_std[j])\n",
        "\n",
        "print(beta_results_df)\n",
        "excel_filename = 'fgls.xlsx'\n",
        "beta_results_df.to_excel(excel_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKgOk-jiZAaH",
        "outputId": "6aef5bb8-c968-4489-f82a-5a0b2ebf6a8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Beta_0                        Beta_1                    \n",
            "N       Media   Mediana    Desvio     Media   Mediana    Desvio\n",
            "1   -3.050148 -3.014695  3.883427  0.804025  0.802498  0.137724\n",
            "2    -3.05791 -3.038393  2.744697  0.801194   0.80043  0.095921\n",
            "6   -3.019007 -3.027394  1.593782   0.80047  0.801474  0.055177\n",
            "20  -2.992133 -2.982704  0.874118  0.799554  0.799518  0.030043\n",
            "40  -3.003301 -3.005471  0.618801  0.800199  0.800226  0.021225\n",
            "100 -3.004554 -3.006429  0.391135   0.80011  0.800009  0.013415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical measures of estimated Beta MCG Cholesky"
      ],
      "metadata": {
        "id": "awXFRpKeZDIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the results of the Beta MCG coefficients.\n",
        "beta_chol_results_df = pd.DataFrame(index=N, columns=beta_columns)\n",
        "\n",
        "for j in N:\n",
        "    beta_chol_results_df.loc[j, ('Beta_0', 'Media')] = np.mean(beta_0_estimates_chol[j])\n",
        "    beta_chol_results_df.loc[j, ('Beta_0', 'Mediana')] = np.median(beta_0_estimates_chol[j])\n",
        "    beta_chol_results_df.loc[j, ('Beta_0', 'Desvio')] = np.mean(beta_0_std_chol[j])\n",
        "\n",
        "    beta_chol_results_df.loc[j, ('Beta_1', 'Media')] = np.mean(beta_1_estimates_chol[j])\n",
        "    beta_chol_results_df.loc[j, ('Beta_1', 'Mediana')] = np.median(beta_1_estimates_chol[j])\n",
        "    beta_chol_results_df.loc[j, ('Beta_1', 'Desvio')] = np.mean(beta_1_std_chol[j])\n",
        "\n",
        "print(beta_chol_results_df)\n",
        "excel_filename = 'cholesky_mcg.xlsx'\n",
        "beta_chol_results_df.to_excel(excel_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HxCpN1kZEts",
        "outputId": "03effa53-878a-40bf-9571-b54c27e723e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Beta_0                        Beta_1                    \n",
            "N       Media   Mediana    Desvio     Media   Mediana    Desvio\n",
            "1   -3.020886 -3.044395  3.740923  0.803312  0.801303  0.134831\n",
            "2   -3.049817 -3.008423   2.30975  0.800829  0.800353  0.081213\n",
            "6   -3.027228 -3.034279  1.238463  0.800669  0.800605  0.042856\n",
            "20  -3.004907 -2.983271  0.663452  0.800011  0.799637  0.022827\n",
            "40  -3.005344 -3.007682   0.46692  0.800258   0.80027  0.016031\n",
            "100 -3.006917  -3.00588  0.294519  0.800176  0.800048  0.010105\n"
          ]
        }
      ]
    }
  ]
}